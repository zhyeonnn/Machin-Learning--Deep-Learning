## 배깅 vs 보팅
### 배깅
- 앙상블 학습 기법(배깅, 부스팅, 스태킹) 중 하나
- 주어진 데이터셋에서 중복을 허용하여 여러 개의 샘플을 만들고 그 샘플마다 독립적으로 모델을 학습시켜 최종 예측값을 결정하는 방식
- 한 종류의 classifier를 여러개 사용

### 보팅
- 여러 종류의 classiffier 사용

### 결정트리
- 전체 데이터를 특정 기준에 따라 몇 개의 영역으로 분류하는 모델
- 리프노드
  트리의  가장 마지막에 위치한 노드
- 내부 노드
  줄간에 위치한 모든 노드
    
## 배깅의 대표적인 알고리즘: 랜덤 포레스트(RandomForestClassifier)

### 랜덤 포레스트
- classiffier로 결정 트리를 사용하는 알고리즘
- 앙상블 알고리즘 중 비교적 빠른 속도
- 높은 예측 성능
- 결정 트리의 쉽고 직관적인 장점
- 랜던 포레스트의 중요 하이퍼파라미터
- n_estimators=100
    ㄴ 생성할 결정트리의 개수 지정 -> 100개의 트리를 학습
        ㄴ 트리는 뭐 하나를 보고 개수를 정하는게 아니기 때문에 기본값 100부터 늘려가면서 성능 확인해야한다
        ㄴ 트리는 정확도, 오차감소, 학습 시간, 메모리, 검증 성능 곡선 등을 결정한다 
        ㄴ 결정트리의 개수가 많을수록 성능이 좋아지지만 너무 높은 값을 지정하면 과적합 문제 또는 학습 수행 시간이 길어질 수 있다.

- get_params()
    ㄴ 하이퍼파라미터 확인, 모델이나 파이프라인의 하이퍼파라미터를 딕셔너리 형태로 반환하는 매서드
        ㄴ 하이퍼파라미터
            ㄴ 모델의 성능에 직접적인 영향을 미친다
            ㄴ 파라미터는 모델이 데이터 학습을 통해 자동으로 얻는 값, 하이퍼파라미터는 사용자가 미리 정해야하고, 학습과정에서 자동으로 갱신되지 않는 값
            ㄴ 하이퍼파라미터는 이미 고정된 설정값이 존재
 - 최소한의 확인할 하이퍼파라미터
     'max_depth': 트리 최대 깊이
     'max_features': 트리마다 고려할 특성 수 -> 과적합 방지에 중요, sqrt = 분류, 1/3 = 회귀
     'min_samples_leaf': 리프에 필요한 최소 샘플 수
     'min_samples_split': 노드를 나눌 최소 샘플 수
     'n_estimators': 트리 개수 -> 많으면 안정적이지만 느림
