#### 인공지능을 구현하는 방법
- 머신러닝, 딥러닝이 있다.

#### 인공지능 > 머신러닝 > 딥러닝
| 분류 |머신러닝|딥러닝|
|:------:|:------:|:---:|
||인공 신경망, 서포트 벡터 머신, 결정 트리|CNN, RNN, RBM|
||규칙을 스스로 수정하지만 데이터를 컴퓨터가 이해할 수 있는 형태로 바꿔주는건 <br>사람이 해줘야한다. -> 스스로 판단할 수 있도록 개발하는 것|대량의 데이터를 신경망에 적용하면 컴퓨터가 <div>스스로 분석한 후 답을 찾는다.</div>|
||범용적인 목적을 위해 제작된 것이기 때문에 <div>데이터 특징을 스스로 추출하지 못한다.</div>||
|동작원리|입력 데이터에 알고리즘을 적용하여 <div>예측을 수행하는 방식으로 동작한다.</div>|정보를 전달하는 신경망을 사용하여 데이터 특징<div> 및 관계를 해석하는 방식이다.</div>|
|재사용|동일한 유형의 데이터 분석을 위한 <div>재사용은 불가능하다.(??)</div>|구현된 알고리즘은 동일한 유형의 데이터를<div> 분석하는 데 재사용된다.</div>|
|데이터|수천개의 데이터가 필요하다.|수백만 개이상의 데이터가 필요하다.|
|훈련 시간|단시간|장시간 훈련할 수 있다.|
|데이터 활용|정형화되어있는 데이터(DB, 엑셀파일 등)|비정형 데이터(이미지, 영상, 텍스트)|
|결과|점수, 분류 등의 숫자값|점수, 텍스트, 소리 등 전부 가능하다.|

**-> 정형화도 딥러닝을 사용할 수 있고, 비정형화도 머신러닝이 사용할 수 있지만 서로 더 잘 할 수 있는 분야로 사용하는 것이 더 좋을 것이다.**

# 머신러닝
- 학습 단계, 예측 단계로 구분된다.

|학습 단계|예측 단계|
|:------:|:------:|
|학습 데이터를 대상으로 학습시키고 결과로 모형이 생성된다.|학습 단계에서 생성된 모형에 새로운 데이터를 적용하여 결과를 예측한다.|

#### 전체 과정 : 데이터를 통한 학습 -> 모형 생성 -> 모형에 새로운 데이터 적용 -> 결과 예측
13p 참고
*특성 추출 : 인간이 컴퓨터가 데이터를 인지할 수 있도록 변환하는 작업, 특성을 인간이 변환하는 작업이다.

- 주요 구성 요소는 데이터와 모델(모형)이다.
- 데이터 수집 후 훈련을 위해 <b>훈련 데이터셋</b>과 <b>검증 데이터셋</b> 용도를 분리해서 사용해야 한다.
- 보통 데이터의 80%는 훈련용, 20%는 검증용으로 분리한다.(이렇게 사용은 하지만 어떠한 데이터가 20%에 몰려있다면 결과가 일반화될 수 있다는 단점이 있기도 하다.)
- 훈련 데이터가 나쁘면 실제 현상의 특성을 제대로 반영할 수 없기 때문에 데이터의 특징이 잘 반영된 훈련 데이터를 확보해야 한다.
- <b>모델(모형)</b>은 학습 단계에서 얻은 최종 결과물이면서 가설이라고도 한다.
17p
- ### 모델(가설)을 선택하여 학습 및 평가하고 평가를 바탕으로 모델을 업데이트한다.

### 머신 러닝의 학습 알고리즘(지도 학습, 비지도 학습, 강화 학습)

<img width="50%" height="550" alt="image" src="https://github.com/user-attachments/assets/1c61eb81-a565-4a64-9aa3-c1695330b42d" /><img width="50%" height="550" alt="image" src="https://github.com/user-attachments/assets/93f7bab7-b714-46e9-a37a-fb3b2f65b2e2" />


|지도 학습|비지도 학습|
|:------:|:------:|
|정답을 알려주고 다음 정답을 예측할 수 있도록 학습시키는 방법이다.|정답을 알려주지 않고 특징(다리가 짧은 초식 동물)이 비슷한 데이터를<div>(토끼, 다람쥐) 클러스터링(범주화)하여 예측하는 방법이다.</div>|
|주어진 데이터에 대해 명확하게 분류가 가능하다.<div>(정확한 정답을 보여준다.)</div>|유사도 기반으로 특징이 유사한 데이터끼리<div>클러스터링으로 묶어서 분류한다.</div> <div>(정답이 하나이상일 수 있다.정답 하나를 선택하는게 아니라 묶음 전체로 분류한다.)</div>|

22p
= 지도학습-> 선 / 비지도학습 -> 원형(묶음)
## 강화 학습
- 자신의 행동에 대한 보상을 받으며 학습을 진행한다.
- ex) 게임(쿠키런 : 쿠키(=게이머)가 에이전트, 게임 배경이 환경이 된다.)에이전트가 환경에 따라 보상을 얻는다.동전이나 젤리를 취득하면 몸집이 커지는 등의 보상을 얻는 것이다.
- 강화학습은 보상이 커지는 행동을 자주 하도록 하고, 보상이 줄어드는 행동은 덜하도록 학습을 진행해야 한다.

## 지도 학습(분류, 회귀)
|분류|회귀|
|:------:|:------:|
|주어진 데이터를 정해진 범주에 따라 분류|데이터들의 특성을 기준으로 연속된 값을 <div>그래프로 표현하여 예측할 때 사용된다.</div>|
|분류|그래프로 표현하여 예측|
|이산형 데이터|연속형 데이터|
|훈련 데이터의 레이블 중 하나를 예측 <div>->데이터 안에 있는 클래스로만 나누어진다.</div>|숫자값으로만 나오고 최대한 정확히 예측|

+추가)
- 회귀는 선에 가까울수록 예측이 잘된 것이다.(선은 모델이 예측한 선, 점은 실제 데이터 위치)

### K-최근접 이웃(K-nn : K-nearest neighbor)
`iris.ipynb`<br>
`k-nn.ipynb`
- 새로운 입력을 받았을 때 기존 클러스터에서 모든 데이터와 인스턴스 기반 거리를 측정한 후 가장 많은 속성을 가진 클러스터에 할당되는 분류 알고리즘이다.
- 즉, 새로운 입력 데이터의 주변에 가장 많은 속성을 가진 분류에 할당된다.

### 서포트 벡터 머신(SVM)
`iris_3.ipnb`
- 분류되지 않은 새로운 데이터가 나타나면 결정 경계를 기준으로 경계의 어느 쪽에 속하는지 분류하는 모델
- 주어진 데이터에 대한 분류
- 커널만 적절히 선택하면 정확도가 좋게 나온다

<img width="40%" height="572" alt="image" src="https://github.com/user-attachments/assets/7f44490f-e4c8-44d1-895e-e21988b78165" />

- 선형, 비선형으로 분류 지원
- 선형으로 분류할 수 없을 때 비선형 분류를 한다.
  #### 비선형 분류의 문제를 해결하는 방법
  - 저차원(평면)에서 고차원(입체)으로 보낸 뒤 나누어 선형으로 분류할 수 있게된다. -> 수학적 계산으로 성능에 문제 발생 가능성 -> 문제 해결을 위해 도입된 <b>'커널 트릭'</b>

<img width="40%" height="743" alt="image" src="https://github.com/user-attachments/assets/4ad996c7-9866-4a96-889b-af21b5c89616" />

  #### 결정 경계
  - 데이터를 분류하기 위한 기준선
  - 결정 경계는 데이터가 분류된 클래스에서 최대한 멀리 떨어져있을 때 성능이 가장 좋다
  #### 마진
  - 결정 경계와 서포트 벡터 사이의 거리
  #### 서포트 벡터
  - 결정 경계와 가까이 있는 데이터들을 의미
  - 최적의 결정 경계는 마진을 최대로 해야 한다.
  #### 하드 마진
  - 이상치를 허용하지 않는 것
  #### 소프트 마진
  - 어느 정도의 이상치들이 마진 안에 포함되는 것

- #### 커널트릭
  #### 선형 커널
  - 선형 모델을 위한 커널
  #### 가우시안 RBF커널, 다항식 커널
  - 비선형을 위한 커널
  - 벡터 내적을 계산한 후 고차원으로 보내는 방법(연산량 줄였음)
  #### 가우시안 RBF커널
  - 입력 벡터를 차원이 무한한 고차원으로 매핑하는 것
  - 가우시안은 차수에 제한없이 무한한 확장이 가능하다

### 결정 트리
<img width="40%" height="413" alt="image" src="https://github.com/user-attachments/assets/b23d25d5-4131-40e6-ace9-05ca22bfa6d9" />

`titanic.iphb`
- 이상치가 많은 값으로 구성된 데이터셋을 다룰 때 사용
-  데이터를 분류하거나 결과값을 예측하는 분석 방법
- 1차 분류에서 순도 증가, 불순도와 불확실성 감소
- #### 순도
  - 일정 범주 안에서 같은 종류의 데이터만 모여있는 상태
- #### 불순도
  - 다른 데이터도 섞여있는 상태
- #### 불확실성을 계산하는 방법
  #### 1. 엔트로피
   - 확률의 불확실성을 수치로 나타낸 것
   - 엔트로피가 높을수록 불확실성이 높다
  #### 2. 지니 계수
   - 불순도를 측정하는 지표
   - 데이터의 통계적 분산 정도를 정량화해서 표현
   - 지니계수가 높을수록 데이터가 분산되어 있음
   - 엔트로피보다 계산이 빨라서 결정 계수에서 많이 사용한다

### 회귀
- 변수 두개가 주어졌을 때 한 변수에서 예측하거나 두 변수의 관계를 규명하는 데 사용

|변수 유형|변수명|
|:------:|:------:|
|영향을 미칠 것(영향을 주는 것)으로 예상되는 변수|독립변수|
|영향을 받을 것으로 예상되는 변수|종속변수|

### 로지스틱 회귀
`digits.py`
- 주어진 데이터에 대한 확신이 없거나 향후 추가적으로 훈력 데이터셋을 수집하여 모델을 훈련시킬 수 있는 환경에서 유용
- 사건의 확률을 확인하는 데 사용
- 각 집단에 속하는 확률의 추정치는 0에서 1사이
- 0.5를 기준으로 분류한다.

|일반적인 회귀분석|로지스틱 회귀분석|
|:------:|:------:|
|연속형 변수|이산형 변수|
|최소제곱법|최대우도법|
|F-테스트, t-테스트|X제곱 테스트|

   - #### 모델 성능 측정하는 방법
     - 혼동행렬(정확도, 정밀도, 재현율)
     - F1-스코어
     - ROC 커브 등  
   - #### 정확도에 대한 성능 확인(score)
     - `logisticRegr.score( )`
   - #### 혼동행렬
     - 결과 확인용으로만 사용하기
   - #### 최소제곱법
     - 실제값에서 예측값을 뺀 후 제곱

<img width="50%" height="736" alt="image" src="https://github.com/user-attachments/assets/101391de-a2d3-41a7-a603-0a713be5d491" />
    
  - #### 최대 우도법
    - 우도(가능도) : 여러 가능한 가설을 평가할 수 있는 척도
    - 최대우도 : 나타난 결과에 해당하는 가설마다 계산된 우도 값 중 가장 큰 값(일어날 가능성이 가장 큰 것)
    - 최대우도법은 즉, 최대우도 추정치, 최대 가능성 추정량을 말한다.

### 선형 회귀
`weather.ipynb`
- 종속 변수와 독립 변수 사이의 관계를 설정하는 데 유용
- 복잡한 연산 과정이 없다
- 즉, 독립 변수를 사용하여 종속 변수의 움직임을 예측
- 독립변수는 1개이상일 수 있다
- 독립변수가 1개이면 단순 선형 회귀
- 독립변수가 2개이상이면 다중 선형 회귀

  - #### weather.csv를 통해서 plot으로 데이터 간 관계를 시각화로 확인할 수 있다.
    ```python
    plt.scatter(X_test, y_test,color='gray')
    plt.plot(X_test,y_pred,color='red',linewidth =2)
    ```
    - 실제값과 예측값을 시각화하여 표현
    - 비슷한 결과가 나온다
    - 루트 평균제곱법을 통한 값으로 정확도를 확인할 수 있다
    - #### 정확도 확인하는 방법
    - 온도의 범위(최소 온도, 최대 온도의 차)를 구해서 10%이하 값보다 낮으면 비슷하게 예측된다는 의미
    ```python
	maxTemp = dataset['MaxTemp'].values.max()
	minTemp = dataset['MinTemp'].values.min()
	tempRange = maxTemp - minTemp
	succesePedict = tempRange * 0.1
	print("10% 기준값 :",succesePedict)
    ```

## 비지도 학습
- 정답이 없는 상태에서 훈련시키는 방식
- 군집, 차원 축소가 있다
- 군집(= 군집화, 클러스터) : 각 데이터의 유사성을 측정해서 유사성이 높은 데이터끼리 집단을 분류하는 것
- 차원 축소 : 차원을 나타내는 특성을 줄여서 데이터를 줄이는 방식

|군집|차원 축소|
|:------:|:------:|
|데이터 그룹화|데이터 간소화|
|K-평균 군집화|주성분 분석|
|예시) 사용자의 관심사에 따라 그룹화하여 마케팅에 활용|예시) 데이터 압축, 중요한 속성 도출|

- #### 유사도 측정 방법
  - 유클리드 거리
  - 맨해튼 거리
  - 민코프스티 거리
  - 코사인 유사도 등
- #### K-평균 군집화
  - 데이터를 입력받아 소수의 그룹으로 묶는 알고리즘
  - 몇개의 클러스터를 구성할지 사전에 알 수 있을 때 사용하면 유용하다.
  - 비선형일 때는 결과가 다르게 발생할 수 있어서 사용하지 않는 게 좋다.
  - 성능은 K값에 따라 달라진다.
  - 반복 횟수에 따른 군집화 과정
  <img width="40%" height="661" alt="image" src="https://github.com/user-attachments/assets/ab8c19f3-1fef-47d0-8880-7b859d210bfd" />

  - 군집 크기가 다를 때
  <img width="40%" height="559" alt="image" src="https://github.com/user-attachments/assets/15f907f0-a5cf-4f89-9889-81ae2aaa2d4f" />

  - 군집마다 밀집도와 거리가 다를 때
  <img width="40%" height="538" alt="image" src="https://github.com/user-attachments/assets/cf73b7b5-f079-468a-9847-98561884ad22" />

- #### 밀도 기반 군집 분석(BDSCAN)
  - 사전에 클러스터의 숫자를 알지 못할 때 사용하기 유용
  - 주어진 데이터에 이상치가 많이 포함되었을 때 사용하기 좋음
  - 일정 밀도 이상을 가진 데이터를 기준으로 군집 형성
  <img width="50%" height="592" alt="image" src="https://github.com/user-attachments/assets/8067d9ac-b6fc-4ebf-ba55-78fa0d06e6d4" />

  - 노이즈에 영향을 받지 않으며 K-평균 군집화에 비해 연산량이 많지만 오목하고 볼록한 부분의 처리하는 데 유용함
  <img width="50%" height="524" alt="image" src="https://github.com/user-attachments/assets/e197ba5d-3c44-42c7-8e8d-5bc6d4f4183a" />

   - 노이즈 : 무작위성 데이터로 전처리 과정에서 제거해야하는 부분
   - 이상치 : 관측된 데이터 범위에서 벗어난 아주 작거나 큰 값
  <img width="40%" height="549" alt="image" src="https://github.com/user-attachments/assets/d59b5811-6a6a-440d-8a92-047f4661c5ac" />

   - #### 밀도 기반 군집 분석을 이용한 군집 방법
  <img width="40%" height="641" alt="image" src="https://github.com/user-attachments/assets/9e85ae20-6dd2-4109-9790-52485331aaed" />
  <img width="40%" height="449" alt="image" src="https://github.com/user-attachments/assets/f5d5376c-8446-46da-adb4-7e9822e56ad8" />

     1. 원 안에 점 p1에서 엡실론 내에 점이 m개 있으면 군집으로 인식
     2. 점들을 가지고 있는 p1이 중심점
     3. m = 3이면 엡실론 내에 3개이상의 점이 있을 때부터 하나의 군집이 된다. -> 군집 생성
     4. 밀도가 높은 비역에서 중심점을 만족하는 데이터가 있다면 그 지역을 포함해서 군집 생성 -> 두 번째 군집 생성, 군집 확대
     5. 더 이상의 중심점을 정의할 수 없을 때까지 <b>군집 생성, 두번째 군집 생성, 군집 확대</b>를 반복
     6. 어떤 군집에도 포함되지 않은 데이터를 노이즈로 정의 -> 노이즈 정의

- #### 주성분 분석 PCA
  - 데이터 간소화
  - 편하게 시각화해서 보기위해 특성을 압축해서 시각화
  - 변수가 많은 고차원 데이터를 저차원으로 축소시켜서 대표 특성만 추출
  - #### 축소 방법
  `creditcard`
    - 데이터들의 분포 특성을 잘 설명하는 벡터 두개 선택(적절한 가중치를 찾을 때까지 학습 진행)
    - 데이터 하나하나에 대한 성분을 분석하는게 아니라 여러 데이터가 하나의 분포를 이룰 때 이 분포의 주성분을 분석하는 것
    ```python
		scaler = StandardScaler() # 평균=0, 표준편차=1 -> 스케일링
		X_scaled = scaler.fit_transform(X)
		
		X_normalized = normalize(X_scaled) # 가우스분포(표준 정규화)에 따르도록 정규화
		X_normalized = pd.DataFrame(X_normalized) # 데이터프레임으로 변환
		
		pca = PCA(n_components=2) # 2차원으로 차원 축소 선언
		X_principal = pca.fit_transform(X_normalized) # 차원 축소 적용
    ```
  - #### 정규화 방법
    - 머신러닝 모델에 주입되는 데이터들을 균일하게 만드는 방법
    - #### Min-Max Normalization(최소-최대 정규화)
      - 모델에 투입될 모든 데이터 중에서 가장 작은 값을 0, 가장 큰 값을 1로 두고, 나머지 값들은 비율을 맞춰서 모두 0과 1 사이의 값으로 스케일링해주는 것
      - 이상치에 취약함
    - #### Z-Score Normalixation(Z-점수 정규화)
      - 표준 정규 분포(가우시안 분포)에 해당하도록 값을 바꿔주는 것
      - 데이터의 표준편차가 크면 정규화 값이 0에 가까워진다
  - 흐름
      
    `스케일링 -> fit,transform -> 정규화 -> DataFrame변환(안 하면 배열로 나열) -> 차원 축소(n_components = ?)
    ->fit, transform(적용) -> DataFrame 변환 -> 열이름 변경 X_principal.colums=['','']`
  - 시각화 자료에서 군집이 잘 표현되면 튜닝 잘한 것

# 딥러닝
- 인간의 신경망 원리를 모방한 심층 신경망 이론을 기반으로 고안된 것이다.
- 인간의 뇌가 엄청난 뉴런과 시냅스로 구성되어 있다. 컴퓨터에도 뉴런과 시냅스 개념을 적용했다.
- 복잡하게 연결된 수많은 뉴런을 병렬 연산하여(동시에 연산이 가능하게) 기존에 컴퓨터가 수행하지 못했던 음성, 영상 인식 등의 처리를 가능하게 하였다.
- 입력층, 출력층, 두개이상의 은닉층으로 구성
- 여러 은닉층을 결합하여 비선형 영역을 표현할 수 있다
- 은닉층 개수가 많을수록 데이터 분류가 잘되고 있음을 볼 수 있다.
  
  <img width="50%" height="263" alt="image" src="https://github.com/user-attachments/assets/21a25d77-2039-4a0a-83af-dd720a29f8f8" />
- 은닉층이 많을수록 문제가 생길 수도 있다
  1. 과적합 : 훈련 데이터를 과하게 학습하여 훈련 데이터에서는 오차가 감소되지만 검증 데이터 즉, 학습되지 않은 새로운 데이터에서는 오차가 증가할 수 있다.
     -> 해결 방법 : 20~30%정도 `dropout`
  2. 기울기 소멸 문제 : 출력층에서 은닉층으로 전달되는 오차가 크게 줄어서 학습이 되지 않는 현상
     -> 해결 방법 : 렐루 활성화 함수 사용
  3. 성능이 나빠지는 문제
     -> 경사하강법은 손실 함수의 비용이 최소가 되는 지점을 찾을 때까지 기울기가 낮은 쪽으로 이동시키는 과정을 반복하는데 이때 성능이 나빠지는 문제가 발생할 수 있다.

     <img width="50%" height="408" alt="image" src="https://github.com/user-attachments/assets/d202e134-d139-4add-b862-90f1fafff00b" />

     - <경사 하강법>
     	<br> **-> 실습(`do it!.ipyb`)**
		<br> 기본 공식 : **y = wx + b**
		1. 임의의 값으로 일단 넣고 확인 -> w = 1.0, b = 1.0
		2. x값에는 입력값을 넣는데 임의로 x[0]값만 넣어서 확인 -> y_hat = x[0] * w + b
		3. 위에서 나오는 y_hat값과 y[0]인 정답과 비교하여 비슷한 값이 나오도록 임의로 설정한 w와 b의 값을 변경하여 수정한다
		4. 변화율이 양수이면 w를 증가시켜 예측값을 증가시킨다.
		4-1. 변화율이 음수이면 w를 감소시켜 예측값을 증가시킨다.

        <img width="1254" height="515" alt="image" src="https://github.com/user-attachments/assets/373523a7-50de-49c2-a5f2-0d1e3bcd623c" />

  
       - 배치 경사 하강법
         - 전체 데이터셋에 대한 오류를 구한 후 기울기를 한 번만 계산하여 모델의 파라미터를 업데이트
         - 단점 : 오래걸림 
       - 확률적 경사 하강법
         - 임의로 선택한 데이터에 대해 기울기를 계산하는 방법
         - 배치 경사 하강법보다 정확도는 낮을 수 있지만 속도가 빠른 장점
         - <b>옵티마이저</b>
         - 학습 속도와 운동량을 조정
         - 옵티마이저 유형(참조 4장68p)
         
         <img width="50%" height="536" alt="image" src="https://github.com/user-attachments/assets/c874582b-2c0f-49cc-8c4f-4a0b47201f69" />

     - 미니 배치 경사 하강법
       - 전체 데이터셋을 미니 배치 여러개로 나누고 미니 배치 한 개마다 기울이를 구한 후 그것의 평균 기울기를 이용해서 모델을 업데이트해서 학습하는 방법
       - 확률적 경사 하강법보다 안정적이고 속도고 빠른 장점
       - 가장 많이 사용
       
    -> 해결 방법 : 미니 배치 경사 하강법
       - 안정적이며 속도도 빠르다
  
- #### 용어
  - <b>활성화함수</b>
    - 전달 함수에서 전달받은 값을 출력하기 전에 일정 기준에 따라 출력값을 변화시키는 비선형 함수
    - 시그모이드, 하이퍼볼릭 탄젠트, 렐루 함수 등
    - #### 시그모이드
      - 선형 함수 결과를 0~1 사이에서 비선형 형태로 변형해줌
      - 로지스틱 회귀와 같은 확률적으로 표현하는 데 사용
      - '기울기 소멸' 문제로 잘 사용하지 않음
    - #### 하이퍼볼릭 탄젠트 함수
      - 선형 함수의 결과가 -1~1사이에서 비선형 형태로 변형
      - 시그모이드에서 결과값의 평균이 0이 아닌 양수가 되는 문제는 해결, '기울기 소멸'문제는 발생
    - #### 렐루 함수
      - 입력(x)이 음수일 때는 0 출력, 양수일 때는 x출력
      - 학습 속도가 빠르고 기울기 소멸 문제 발생하지 않음
      - 음수 값을 입력받으면 항상 0을 출력 -> 리키 렐루 함수 사용
    - #### 리키 렐루 함수
      - 음수 값을 입력받으면 0이 아닌 0.001처럼 매우 작은 수 반환
    - #### 소프트맥스 함수
      - 입력값을 0~1사이에 출력
      - 정규화하여 출력 값들의 합이 항상 1이 되도록 함
      - 딥러닝에서 출력 노드의 활성화 함수로 많이 사용
    - #### 손실 함수(평균 제곱 오차, 크로스 엔트로피 오차)
      - 손실함수는 예측된 결과값과 정답의 값을 비교한 것
      - 값이 클수록 많이 틀렸다는 의미, 0에 가까우면 완벽하게 추정할 수 있다는 의미
      - 경사 하강법 : 학습률과 손실 함수의 순간 기울기를 이용하여 가중치를 업데이트
      - 미분의 기울기를 이용하여 오차를 비교하고 최소화하는 방향으로 이동
     
      
       <평균 제곱 오차>
      - 실제 값과 예측 값의 차이를 제곱하여 평균낸 것
      - 값이 작을수록 예측력이 좋음

        <크로스 엔트로피 오차>
      - 분류문제에서 원핫인코딩했을 때만 사용할 수 있는 계산법
      - 손실 함수의 값은 0이 가장 이상적
      - 0값을 찾는건 현실적으로 쉽지않음 -> 가장 작은 값을 찾았다고 생각되는 지점을 <b>지역 최소점</b>이라고 한다
      - 지역 최소점 중에 가장 작은 값을 갖는 지점을 정답으로 간주하고 <b>전역 최소점</b>이라고 한다
      <img width="50%" height="419" alt="image" src="https://github.com/user-attachments/assets/299aad53-6777-4d56-b0fc-5ad60574d6b9" />

## 심층 신경망(DNN)
- 목적에 따라 합성곱 신경망(CNN), 순환 신경망(RNN), 제한된 볼츠만머신(RBN), 심층 신뢰 신경망(DBN)으로 분류
- 심층 신경망은 다수의 은닉층을 추가했기 때문에 별도의 트릭없이 비선형 분류 가능
  ### 합성곱 신경망 CNN
  - 합성곱층과 풀링층(압축)을 포함하는 이미지 처리 성능이 좋은 인공 신경망 알고리즘
  - 영상 및 사진이 포함된 이미지 데이터에서 객체를 탐색하거나 객체 위치를 찾아내는 데 유용
  - 이미지에서 객체, 얼굴, 장면을 인식하기 위해 패턴을 찾는 데 특히 유용함
  - 이미지 전체를 한번에 계산하는게 아니라 이미지의 국소적 부분을 계산하여 시간과 자원을 절약하여 세밀한 부분까지 분석할 수 있다.
 
  - 이미지나 영상을 처리하는 데 유용
  - 이미지 분석은 배열을 펼쳐서 각 픽셀에 가중치를 곱해 은닉층으로 전달하게 됨 -> 펼치게 되면 이미지 데이터의 공간적 구조를 무시하게 됨 -> 방지할 수 있는 법 : 합성곱층

  <img width="30%" height="559" alt="image" src="https://github.com/user-attachments/assets/70d5bea9-c90b-44e4-ad5a-971000a0ecc1" />

  - #### 합성곱 신경망 구조
    - 다차원 배열 데이터를 처리하도록 구성
    - 컬러 이미지같은 다차원 배열 처리에 특화되어있음
    - 계층 구성 -> 입력층, 합성곱층, 풀링층, 완전연결층, 출력층
    
    <img width="50%" height="259" alt="image" src="https://github.com/user-attachments/assets/dea33982-1cfc-49e7-be00-372832d9ee66" />

  - #### 입력층
    - 이미지는 단순 1차원의 데이터가 아닌 3차원의 데이터를 가짐
    - 채널은 이미지가 그레이스케일이면 1, 컬러 RGB면 3값을 가짐
    - *그레이스케일 : 이미지나 사진에서 밝고 어두움의 정도를 나타내는 색상 범위
  - #### 합성곱층
    - 이미지가 들어왔을 때 이미지에 대한 특성을 감지하기 위해 커널이나 필터를 사용
    - 커널/필터는 이미지의 모든 영역을 훑으면서 특성 추출, 추출된 결과를 특성 맵(feature map)
    - 커널은 스트라이드(stride)라는 지정된 간격에 따라 순차적으로 이동

      그레이스케일을 통한 계산법
    - 입력 이미지와 필터를 포개놓고 대응되는 숫자끼리 곱한 후 모두 더하기
    - 아래 사진처럼 옆으로 이동(사진에서는 그레이스케일에 대한 이미지)

      <img width="30%" height="405" alt="image" src="https://github.com/user-attachments/assets/545f4f68-93e7-475c-975f-6b7d19c26ea9" />
      <img width="30%" height="396" alt="image" src="https://github.com/user-attachments/assets/d8372336-8e46-42f1-8ef9-4d425b026b9d" />

      컬러를 사용한 계산법
    - 컬러 RGB는 필터채널이 3이지만 숫자가 3이고, 필터 개수느 1개이다.
    - 이미지와 필터 채널의 채널 개수는 동일하게 맞춰야 한다.

      <img width="30%" height="399" alt="image" src="https://github.com/user-attachments/assets/04f71c62-2961-4745-a152-6c2a822a62e7" />
      <img width="30%" height="537" alt="image" src="https://github.com/user-attachments/assets/51203144-fb06-4894-a06c-9b2ece6099ae" />

   - #### 풀링층
     - 합성곱층과 유사하게 특성 맵의 차원을 다운 샘플링하여 연산량을 감소, 주요 벡터를 추출하여 학습을 효과적으로
     - 다운 샘플링 : 이미지 축소
     - 풀링 연산 : 대상 영역에서 최댓값 추출하는 <b>최대 풀링</b>, 대상 영역에서 평균을 반환하는 <b>평균 풀링</b>
     - 대부분의 합성곱 신경망에서는 최대 풀링 사용
     - 평균 풀링을 사용하게되면 각 커널 값을 평균화시켜 중요한 가중치를 갖는 값의 특성이 희미해질 수 있기 때문
     - #### 최대 풀링
       - 풀링하는 값들 중에서 최댓값을 선택
       - 스트라이드가 2이기 때문에 2칸 간격으로 이동

      <img width="30%" height="276" alt="image" src="https://github.com/user-attachments/assets/0ff9b983-e0b1-4a1e-9526-dc0446eb9fc0" />
 
    - #### 평균 풀링
      - 최대 풀링과 같은 방식으로 진행하지만 최대 값 대신 평균 값을 선택

    - #### 완전연결층
      - 차원이 축소된 특성 맵이 완전연결층으로 전달
      - 이미지는 3차원에서 1차원 벡터로 펼치게 됨
        
        <img width="30%" height="446" alt="image" src="https://github.com/user-attachments/assets/c08d033f-9950-4283-a6d1-3b4f5735f442" />

    - #### 출력층
      - 소프트맥스 활성화 함수 사용됨, 입력받은 값을 0~1 사이 값으로 출력
      - 소프트맥스 함수를 사용하면 이미지가 각 레이블에 속할 확률 값이 출력 -> 가장 높은 확률 값을 갖는 레이블이 최종 값으로 선정

  - #### 1D,2D,3D

  - #### 흐름
    - 합성곱층과 풀링층을 거치면서 입력 이미지의 주요 특성 벡터를 추출
    - 추출된 벡터들은 완전연결층을 거쳐 1차원 벡터로 변환
    - 마지막 출력층에서 활성화 함수인 소프트맥스함수를 사용하여 최종 결과 출력

    기존 신경망과의 차별점
  - 각 층의 입출력 형상 유지
  - 이미지 공간 정보를 유지하면서 인접 이미지와 차이가 있는 특징을 인식
  - 복수 필터로 이미지 특징 추출하고 학습
  - 추출한 이미지 특징을 모으고 강화하는 풀링층이 있다
  - 학습 파라미터가 매우 적다

  ### 순환 신경망 RNN
  - 시계열 데이터(음악, 영상 등) 같은 시간 흐름에 따라 변화하는 데이터를 학습하기 위한 인공 신경망
  - 순환 = 자기 자신을 참조한다는 것, 현재 겨로가가 이전 결과와 연관이 있다는 의미
  
    특징
  - 데이터에 시간성을 가진 데이터가 많다
  - 시간성 정보를 이용하여 데이터의 특성을 다룬다
  - 시간에 따라 내용이 변해서 데이터가 동적임, 길이가 가변적
  - 대표적인 예시 : 언어 모델링, 텍스트 생성, 자동 번역, 음성 인식, 이미지 캡션 생성 등

  ### 제한된 볼프만 머신 RBN
  - 가시층과 은닉층으로 구성
  
    특징
  - 차원 감소, 분류, 특성 값 학습, 선형 회귀 분석, 주제 모델링에 사용
  - 기울기 소멸 문제 해결

  ### 심층 신뢰 신경망
  - 사전에 훈련된 RBN을 여러 층으로 쌓은 형태로 연결된 신경망
  - 레이블이 없는 데이터에 대한 비지도 학습이 가능
  - 부분적인 이미지에서 전체를 연상하는 일반화와 추상화 과정을 구현할 때 사용하기 유용
  <img width="50%" height="328" alt="image" src="https://github.com/user-attachments/assets/87fdd353-c493-44dc-aacb-e87380008394" />

    특징
  - 심층 신뢰 신경망을 학습시켜 가면서 계층적 구조를 생성
  - 비지도 학습으로 학습
  - 위로 올라갈수록 추상적 특성을 추출

  
### 딥러닝의 학습과정
  - #### 순전파, 역전파
  - 네트워크에 훈련 데이터가 들어올 때 발생, 데이터를 기반으로 예측값을 계산하기 위해 전체 신경망을 교차해 지나간다
    1. 입력 데이터에 대해 연산을 거쳐 결과값을 내는 과정 -> <b>순전파</b>
    2. 그 결과값을 실제 값과 예측 값의 차이(오차)를 최소화하여 오차값을 줄어드는 방향으로 가중치를 변경한다
    3. 결과값에서부터 가중치까지 정보가 역방향으로 전달된다 -> <b>역전파</b>
  - 역전파는 계산 과정이 복잡하고 많은 자원이 요구된다, 계산시간도 오래 걸린다. 그 해결방법이 합성곱 신경망(CNN)

#### 데이터 준비 방법(데이터를 쉽게 구할 수 있는 방법)
1. 텐서플로(tnesorflow)나 캐라스(keras)에서 제공하는 데이터셋을 사용하는 것이다.
  - 제공되는 데이터들은 이미 전처리가 되어있기 때문에 바로 사용할 수 있다.
2. 캐글(kaggle)같은 곳에 공개된 데이터를 사용하는 것이다.
  - 국내의 공개 데이터들도 사용할 수 있으나 상당히 많은 전처리를 해야하기 때문에 캐글같은 플랫폰에서 제공되는 데이터를 활용하면 사용하기가 편리하다.
  - 캐글의 단점은 공용데이터이기 때문에 데이터들을 많이 공개하지 않아서 데이터 양이 적다.

#### 모델 정의
- 모델 정의 단계에서 신경망을 생성한다.
- 은닉층의 개수가 많을수롣 성능은 좋아지지만 과적합이 발생할 확률이 높다.(은닉층과 과접합은 서로 관계가 있다.)

#### 모델 컴파일
- 컴파일 단계에서 데이터 형태에 따라 활성화 함수, 손실 함수, 옵티마이저를 선택할 수 있다.
- 훈련 데이터셋 형태가 연속형이면 평균 제곱 오차
- 훈련 데이터셋 형태가 이진 분류라면 크로스 엔트로피
- 과접합을 피할 수 있는 활성화 함수 및 옵티마이저 선택이 중요하다

#### 모델 훈련
- 훈련 단계에서는 한 번에 처리할 데이터양을 지정한다.
- 한 번에 처리할 데이터양이 많아지면학습 속도가 느려지고 메모리 부족 문제가 발생할 수 있다.(적당한 데이터양을 선택하는 것이 중요하다.)
- 일정한 묶음으로 나누어 처리할 수 있는 "배치"와 훈련 횟수를 정하는 "에포크"가 중요하다.
- 훈련 과정에서 값의 변화를 시각적으로 표현할 수 있도록 파라미터와 하이퍼파라미터에 대한 최적의 값을 찾을 수 있어야 한다.
- 훈련 데이터셋 1000개에 대한 배치 크기가 20이라면 샘플 단위 20개마다 모델 가중치를 한 번씩 업데이트 시킨다는 의미 = 1000개의 데이터가 있는데 배치 크기 즉 20개의 칸을 만들었고 20개마다 모델 가중치를 1번씩 업데이트, 20->1 / 20->1+1=2 ... 20 -> 49+1=50(1000/20)이니까 20개씩 1번 가중치 업데이트면 50번 가중치가 업데이트 된다.
- 그리고 에포크가 10이고 배치 크기가 20이라면 50번의 가중치 업데이트 하는 것을 10번 반복한다는 것이다.(즉, 500번 업데이트)

34p

성능이 좋다 = 예측을 잘한다, 정확도가 높다, 훈련 속도가 빠르다.

#### 모델 예측
- 검증 데이터셋을 생성한 모델에 적용하여 실제로 예측을 진행하는 단계(예측력이 낮다면 파라미터를 튜닝하거나 신경망 자체를 재설계해야 할 수도 있다.)


#### 핵심 구성 요소 
- 신경망, 역전파

#### 머신러닝과의 차이
- 심층 신경망을 가지고 있다는 점에서 차이가 발생한다.
- 심층 신경망에는 데이터셋의 어떤 특성들이 중요한지 스스로에게 가르쳐줄 수 있는 기능이 있다.
- 가중치 값을 업데이트하기 위한 역전파가 중요하다.
- 역전파 계산 과정에서 사용되는 미분이(오차를 각 가중치로 미분) 성능에 영향을 미치는 주요한 요소이다.
- 텐서플로를 이용하면 딥러닝 알고리즘 구현이 간단하고 편리하다.

39 40p

## 딥러닝 학습 알고리즘(지도 학습, 비지도 학습, 전이 학습, 강화 학습)
## 지도 학습
- 가장 많이 사용되는 것은 합성곱 신경망이다.
- 합성곱 신경망을 목적에 따라 이미지 분류, 이미지 인식, 이미지 분할로 분류한다.

|이미지 분류|이미지 인식|이미지 분할|
|:---:|:---:|:---:|
|이미지를 알고리즘에 입력하면 그 이미지가 어떤 클래스 레이블에 속하는지 알려주기 때문에 이미지 데이터를 유사한 것끼리 분류할 수 있다.|사진을 분석하여 그 안에 있는 사물의 종류를 인식하는 것이다.<div> ex) 의료 이미지에서 질병을 식별</div>|다양한 의료 영상에서 분할된 이미지 정보를 활용한다.|

- 시계열 데이터를 분류할 때는 순환 신경망을 사용한다.
- 시간에 따른 데이터가 있을 때는 순환 신경망을 사용하지만 역전파 과정에서는 기울기가 소멸하는 문제가 발생한다.
- 이때는 게이트를 세 개 추가한 것이 LSTM이다.(망각 게이트, 입력 게이트, 출력 게이트) -> 가장 활발히 사용되고 있다.

## 비지도 학습(워드 임베딩, 군집)
- 컴퓨터가 이해할 수 있게 자연어를 적절하게 변환해줘야 한다.
  ### 워드 임베딩
    - 단어를 벡터로 표현할 수 있다.
    - 단어 의미를 벡터화하는 워드투벡터와 글로브를 가장 많이 사용한다.
    - 번역이나 음성 인식 등 서비스에서 사용한다.
  ### 군집
    - 아무런 정보가 없는 상태에서 데이터를 분류하는 방법
    - 클러스터 안의 데이터는 매우 비슷하고, 다른 클러스터의 데이터와는 구분되도록 나누는 것이 목표이다.
      
## 전이 학습(전이 학습, 사전 학습 모델)
  
  ### 전이 학습
 - 사전에 학습이 완료된 모델을 가지고 원하는 학습에 미세 조정 기법을 이용하여 학습시키는 방법이다.
  ### 사전 학습 모델
 - 사전 학습 모델은 풀고자 하는 문제와 비슷하면서 많은 데이터로 이미 학습이 되어 있는 모델이다.
 - VGG, 인셉션, ModileNet 같은 사전 학습 모델을 사용하면 효율적인 학습이 가능하다.


|머신러닝|딥러닝|
|:---:|:---:|
|간단한 선형 회귀 분류로 원하는 값 도출할 수 있을 때|복잡한 비선형 데이터에 대한 분류 및 예측을 도출하고 싶을 때|
